=== Running /cygdrive/c/repo/cntk_github6/CNTK/x64/debug/cntk.exe configFile=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/TIMIT_TrainWithPreTrain.cntk currentDirectory=D:\TestPreparation\Speech\ASR RunDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu DataDir=D:\TestPreparation\Speech\ASR ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config OutputDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu DeviceId=-1 timestamping=true LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib ScpDir=D:\TestPreparation\Speech\ASR MlfDir=D:\TestPreparation\Speech\ASR NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config ExpDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp DeviceNumber=-1 SGD=[maxEpochs=1] SGD=[epochSize=2048] TIMIT_Train3=[SGD=[maxEpochs=1]] TIMIT_Train3=[SGD=[epochSize=2048]] reader=[verbosity=0] numMBsToShowResult=5
-------------------------------------------------------------------
Build info: 

		Built time: Apr  6 2016 11:16:59
		Last modified date: Tue Apr  5 15:50:31 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: yes
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
		CUB_PATH: c:\Tools\cub-1.4.1\
		CUDNN_PATH: c:\Tools\cudnn-4.0\cuda
		Build Branch: eldak/addingTimitExamplesToTests
		Build SHA1: 40db0b8fcc1e6ffd89b9357a636439c63130b589 (modified)
		Built by eldak on ELDAK-0
		Build Path: c:\repo\cntk_github6\CNTK\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to D:\TestPreparation\Speech\ASR
04/06/2016 10:20:59: -------------------------------------------------------------------
04/06/2016 10:20:59: Build info: 

04/06/2016 10:20:59: 		Built time: Apr  6 2016 11:16:59
04/06/2016 10:20:59: 		Last modified date: Tue Apr  5 15:50:31 2016
04/06/2016 10:20:59: 		Build type: Debug
04/06/2016 10:20:59: 		Build target: GPU
04/06/2016 10:20:59: 		With 1bit-SGD: yes
04/06/2016 10:20:59: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
04/06/2016 10:20:59: 		CUB_PATH: c:\Tools\cub-1.4.1\
04/06/2016 10:20:59: 		CUDNN_PATH: c:\Tools\cudnn-4.0\cuda
04/06/2016 10:20:59: 		Build Branch: eldak/addingTimitExamplesToTests
04/06/2016 10:20:59: 		Build SHA1: 40db0b8fcc1e6ffd89b9357a636439c63130b589 (modified)
04/06/2016 10:20:59: 		Built by eldak on ELDAK-0
04/06/2016 10:20:59: 		Build Path: c:\repo\cntk_github6\CNTK\Source\CNTK\
04/06/2016 10:20:59: -------------------------------------------------------------------

04/06/2016 10:20:59: Running on ELDAK-0 at 2016/04/06 10:20:59
04/06/2016 10:20:59: Command line: 
C:\repo\cntk_github6\CNTK\x64\debug\cntk.exe  configFile=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/TIMIT_TrainWithPreTrain.cntk  currentDirectory=D:\TestPreparation\Speech\ASR  RunDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu  DataDir=D:\TestPreparation\Speech\ASR  ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  OutputDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu  DeviceId=-1  timestamping=true  LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib  ScpDir=D:\TestPreparation\Speech\ASR  MlfDir=D:\TestPreparation\Speech\ASR  NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  ExpDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp  DeviceNumber=-1  SGD=[maxEpochs=1]  SGD=[epochSize=2048]  TIMIT_Train3=[SGD=[maxEpochs=1]]  TIMIT_Train3=[SGD=[epochSize=2048]]  reader=[verbosity=0]  numMBsToShowResult=5



04/06/2016 10:20:59: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
04/06/2016 10:20:59: command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
precision=float
ndlMacros=$NdlDir$\default_macros.ndl
deviceId=$DeviceNumber$
traceLevel=1
SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
]
reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=$ScpDir$\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=$MlfDir$\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=$MlfDir$\TIMIT.statelist
      ]
]
TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=$ExpDir$\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
]
TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=$ExpDir$\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=$MelDir$\add_layer.mel
]
TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
]
TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=$ExpDir$\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=$MelDir$\add_layer.mel
]
TIMIT_Train3=[
    action=train
    modelPath=$ExpDir$\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
]
currentDirectory=D:\TestPreparation\Speech\ASR
RunDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu
DataDir=D:\TestPreparation\Speech\ASR
ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
OutputDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu
DeviceId=-1
timestamping=true
LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
ScpDir=D:\TestPreparation\Speech\ASR
MlfDir=D:\TestPreparation\Speech\ASR
NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
ExpDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp
DeviceNumber=-1
SGD=[maxEpochs=1]
SGD=[epochSize=2048]
TIMIT_Train3=[SGD=[maxEpochs=1]]
TIMIT_Train3=[SGD=[epochSize=2048]]
reader=[verbosity=0]
numMBsToShowResult=5

04/06/2016 10:20:59: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

04/06/2016 10:20:59: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
04/06/2016 10:20:59: command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
precision=float
ndlMacros=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\default_macros.ndl
deviceId=-1
traceLevel=1
SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
]
reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=D:\TestPreparation\Speech\ASR\TIMIT.statelist
      ]
]
TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]
TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]
TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]
TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]
TIMIT_Train3=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
]
currentDirectory=D:\TestPreparation\Speech\ASR
RunDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu
DataDir=D:\TestPreparation\Speech\ASR
ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
OutputDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu
DeviceId=-1
timestamping=true
LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
ScpDir=D:\TestPreparation\Speech\ASR
MlfDir=D:\TestPreparation\Speech\ASR
NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
ExpDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp
DeviceNumber=-1
SGD=[maxEpochs=1]
SGD=[epochSize=2048]
TIMIT_Train3=[SGD=[maxEpochs=1]]
TIMIT_Train3=[SGD=[epochSize=2048]]
reader=[verbosity=0]
numMBsToShowResult=5

04/06/2016 10:20:59: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

04/06/2016 10:20:59: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: TIMIT_TrainWithPreTrain.cntk:command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
configparameters: TIMIT_TrainWithPreTrain.cntk:ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:currentDirectory=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:DataDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:deviceId=-1
configparameters: TIMIT_TrainWithPreTrain.cntk:DeviceNumber=-1
configparameters: TIMIT_TrainWithPreTrain.cntk:ExpDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp
configparameters: TIMIT_TrainWithPreTrain.cntk:LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
configparameters: TIMIT_TrainWithPreTrain.cntk:MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:MlfDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:ndlMacros=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\default_macros.ndl
configparameters: TIMIT_TrainWithPreTrain.cntk:numMBsToShowResult=5
configparameters: TIMIT_TrainWithPreTrain.cntk:OutputDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu
configparameters: TIMIT_TrainWithPreTrain.cntk:precision=float
configparameters: TIMIT_TrainWithPreTrain.cntk:reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=D:\TestPreparation\Speech\ASR\TIMIT.statelist
      ]
] [verbosity=0]

configparameters: TIMIT_TrainWithPreTrain.cntk:RunDir=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu
configparameters: TIMIT_TrainWithPreTrain.cntk:ScpDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
] [maxEpochs=1] [epochSize=2048]

configparameters: TIMIT_TrainWithPreTrain.cntk:timestamping=true
configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_Train3=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
] [SGD=[maxEpochs=1]] [SGD=[epochSize=2048]]

configparameters: TIMIT_TrainWithPreTrain.cntk:traceLevel=1
04/06/2016 10:20:59: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
04/06/2016 10:20:59: Commands: TIMIT_DiscrimPreTrain1 TIMIT_AddLayer2 TIMIT_DiscrimPreTrain2 TIMIT_AddLayer3 TIMIT_Train3
04/06/2016 10:20:59: Precision = "float"
04/06/2016 10:20:59: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
04/06/2016 10:20:59: CNTKCommandTrainInfo: TIMIT_DiscrimPreTrain1 : 1
04/06/2016 10:20:59: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
04/06/2016 10:20:59: CNTKCommandTrainInfo: TIMIT_DiscrimPreTrain2 : 1
04/06/2016 10:20:59: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
04/06/2016 10:20:59: CNTKCommandTrainInfo: TIMIT_Train3 : 1
04/06/2016 10:20:59: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3

04/06/2016 10:20:59: ##############################################################################
04/06/2016 10:20:59: #                                                                            #
04/06/2016 10:20:59: # Action "train"                                                             #
04/06/2016 10:20:59: #                                                                            #
04/06/2016 10:20:59: ##############################################################################

04/06/2016 10:20:59: CNTKCommandTrainBegin: TIMIT_DiscrimPreTrain1
NDLBuilder Using CPU
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:21:08: Creating virgin network.

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 19 nodes to process in pass 1.


Validating network. 13 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L1.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:21:08: Created model with 19 nodes on CPU.

04/06/2016 10:21:08: Training criterion node(s):
04/06/2016 10:21:08: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:21:08: Evaluation criterion node(s):

04/06/2016 10:21:08: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

04/06/2016 10:21:08: Precomputing --> 3 PreCompute nodes found.

04/06/2016 10:21:08: 	featNorm.xMean = Mean()
04/06/2016 10:21:08: 	featNorm.xStdDev = InvStdDev()
04/06/2016 10:21:08: 	logPrior.Prior = Mean()
minibatchiterator: epoch 0: frames [0..1124823] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:29:44: Precomputing --> Completed.


04/06/2016 10:29:44: Starting Epoch 1: learning rate per sample = 0.000391  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses

04/06/2016 10:29:44: Starting minibatch loop.
04/06/2016 10:29:45:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  5.28307190; EvalErr[0]PerSample = 0.99609375; TotalTime = 1.4321s; SamplesPerSecond = 893.8
04/06/2016 10:29:46: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 5.2337179; TotalSamplesSeen = 2048; EvalErrPerSample = 0.98779297; AvgLearningRatePerSample = 0.00039062501; EpochTime=2.21593
04/06/2016 10:29:46: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn'
04/06/2016 10:29:46: CNTKCommandTrainEnd: TIMIT_DiscrimPreTrain1

04/06/2016 10:29:46: Action "train" complete.


04/06/2016 10:29:46: ##############################################################################
04/06/2016 10:29:46: #                                                                            #
04/06/2016 10:29:46: # Action "edit"                                                              #
04/06/2016 10:29:46: #                                                                            #
04/06/2016 10:29:46: ##############################################################################


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 19 nodes to process in pass 1.


Validating network. 13 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L1.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 12 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


04/06/2016 10:29:47: Action "edit" complete.


04/06/2016 10:29:47: ##############################################################################
04/06/2016 10:29:47: #                                                                            #
04/06/2016 10:29:47: # Action "train"                                                             #
04/06/2016 10:29:47: #                                                                            #
04/06/2016 10:29:47: ##############################################################################

04/06/2016 10:29:47: CNTKCommandTrainBegin: TIMIT_DiscrimPreTrain2
NDLBuilder Using CPU
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:29:54: Starting from checkpoint. Loading network from 'F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0'.

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 16 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:29:55: Loaded model with 24 nodes on CPU.

04/06/2016 10:29:55: Training criterion node(s):
04/06/2016 10:29:55: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:29:55: Evaluation criterion node(s):

04/06/2016 10:29:55: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
04/06/2016 10:29:55: No PreCompute nodes found, skipping PreCompute step.

04/06/2016 10:29:55: Starting Epoch 1: learning rate per sample = 0.000391  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:30:20: Starting minibatch loop.
04/06/2016 10:30:22:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  5.07689781; EvalErr[0]PerSample = 0.96015625; TotalTime = 1.9312s; SamplesPerSecond = 662.8
04/06/2016 10:30:23: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 5.053731; TotalSamplesSeen = 2048; EvalErrPerSample = 0.95507813; AvgLearningRatePerSample = 0.00039062501; EpochTime=28.5887
04/06/2016 10:30:23: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn'
04/06/2016 10:30:23: CNTKCommandTrainEnd: TIMIT_DiscrimPreTrain2

04/06/2016 10:30:24: Action "train" complete.


04/06/2016 10:30:24: ##############################################################################
04/06/2016 10:30:24: #                                                                            #
04/06/2016 10:30:24: # Action "edit"                                                              #
04/06/2016 10:30:24: #                                                                            #
04/06/2016 10:30:24: ##############################################################################


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 16 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 29 nodes to process in pass 1.


Validating network. 15 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L3.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L3.BFF.FF.T = Times (L3.BFF.W, L2.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L3.BFF.B = LearnableParameter() :  -> [512]
Validating --> L3.BFF.FF.P = Plus (L3.BFF.FF.T, L3.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L3.S = Sigmoid (L3.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L3.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


04/06/2016 10:30:24: Action "edit" complete.


04/06/2016 10:30:24: ##############################################################################
04/06/2016 10:30:24: #                                                                            #
04/06/2016 10:30:24: # Action "train"                                                             #
04/06/2016 10:30:24: #                                                                            #
04/06/2016 10:30:24: ##############################################################################

04/06/2016 10:30:24: CNTKCommandTrainBegin: TIMIT_Train3
NDLBuilder Using CPU
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:30:32: Starting from checkpoint. Loading network from 'F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0'.

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 29 nodes to process in pass 1.


Validating network. 19 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L3.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L3.BFF.FF.T = Times (L3.BFF.W, L2.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L3.BFF.B = LearnableParameter() :  -> [512]
Validating --> L3.BFF.FF.P = Plus (L3.BFF.FF.T, L3.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L3.S = Sigmoid (L3.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L3.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:30:32: Loaded model with 29 nodes on CPU.

04/06/2016 10:30:32: Training criterion node(s):
04/06/2016 10:30:32: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:30:32: Evaluation criterion node(s):

04/06/2016 10:30:32: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
04/06/2016 10:30:32: No PreCompute nodes found, skipping PreCompute step.

04/06/2016 10:30:32: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:30:58: Starting minibatch loop.
04/06/2016 10:31:01:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  4.96834183; EvalErr[0]PerSample = 0.96171875; TotalTime = 2.5203s; SamplesPerSecond = 507.9
04/06/2016 10:31:02: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 4.9595852; TotalSamplesSeen = 2048; EvalErrPerSample = 0.95507813; AvgLearningRatePerSample = 0.003125; EpochTime=30.0529
04/06/2016 10:31:02: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406112058.112406\Examples\Speech\TIMIT_TrainWithPreTrain@debug_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn'
04/06/2016 10:31:03: CNTKCommandTrainEnd: TIMIT_Train3

04/06/2016 10:31:03: Action "train" complete.

04/06/2016 10:31:03: __COMPLETED__