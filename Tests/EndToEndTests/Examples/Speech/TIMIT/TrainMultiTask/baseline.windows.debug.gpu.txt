=== Running /cygdrive/c/repo/cntk_github6/CNTK/x64/debug/cntk.exe configFile=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/TIMIT_TrainWithPreTrain.cntk currentDirectory=D:\TestPreparation\Speech\ASR RunDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu DataDir=D:\TestPreparation\Speech\ASR ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config OutputDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu DeviceId=0 timestamping=true LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib ScpDir=D:\TestPreparation\Speech\ASR MlfDir=D:\TestPreparation\Speech\ASR NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config ExpDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp DeviceNumber=0 SGD=[maxEpochs=1] SGD=[epochSize=2048] TIMIT_Train3=[SGD=[maxEpochs=1]] TIMIT_Train3=[SGD=[epochSize=2048]] reader=[verbosity=0] numMBsToShowResult=5
-------------------------------------------------------------------
Build info: 

		Built time: Apr  6 2016 11:16:59
		Last modified date: Tue Apr  5 15:50:31 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: yes
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
		CUB_PATH: c:\Tools\cub-1.4.1\
		CUDNN_PATH: c:\Tools\cudnn-4.0\cuda
		Build Branch: eldak/addingTimitExamplesToTests
		Build SHA1: 40db0b8fcc1e6ffd89b9357a636439c63130b589 (modified)
		Built by eldak on ELDAK-0
		Build Path: c:\repo\cntk_github6\CNTK\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to D:\TestPreparation\Speech\ASR
04/06/2016 10:17:32: -------------------------------------------------------------------
04/06/2016 10:17:32: Build info: 

04/06/2016 10:17:32: 		Built time: Apr  6 2016 11:16:59
04/06/2016 10:17:32: 		Last modified date: Tue Apr  5 15:50:31 2016
04/06/2016 10:17:32: 		Build type: Debug
04/06/2016 10:17:32: 		Build target: GPU
04/06/2016 10:17:32: 		With 1bit-SGD: yes
04/06/2016 10:17:32: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
04/06/2016 10:17:32: 		CUB_PATH: c:\Tools\cub-1.4.1\
04/06/2016 10:17:32: 		CUDNN_PATH: c:\Tools\cudnn-4.0\cuda
04/06/2016 10:17:32: 		Build Branch: eldak/addingTimitExamplesToTests
04/06/2016 10:17:32: 		Build SHA1: 40db0b8fcc1e6ffd89b9357a636439c63130b589 (modified)
04/06/2016 10:17:32: 		Built by eldak on ELDAK-0
04/06/2016 10:17:32: 		Build Path: c:\repo\cntk_github6\CNTK\Source\CNTK\
04/06/2016 10:17:32: -------------------------------------------------------------------

04/06/2016 10:17:32: Running on ELDAK-0 at 2016/04/06 10:17:32
04/06/2016 10:17:32: Command line: 
C:\repo\cntk_github6\CNTK\x64\debug\cntk.exe  configFile=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/TIMIT_TrainWithPreTrain.cntk  currentDirectory=D:\TestPreparation\Speech\ASR  RunDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu  DataDir=D:\TestPreparation\Speech\ASR  ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  OutputDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu  DeviceId=0  timestamping=true  LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib  ScpDir=D:\TestPreparation\Speech\ASR  MlfDir=D:\TestPreparation\Speech\ASR  NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  ExpDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp  DeviceNumber=0  SGD=[maxEpochs=1]  SGD=[epochSize=2048]  TIMIT_Train3=[SGD=[maxEpochs=1]]  TIMIT_Train3=[SGD=[epochSize=2048]]  reader=[verbosity=0]  numMBsToShowResult=5



04/06/2016 10:17:32: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
04/06/2016 10:17:32: command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
precision=float
ndlMacros=$NdlDir$\default_macros.ndl
deviceId=$DeviceNumber$
traceLevel=1
SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
]
reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=$ScpDir$\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=$MlfDir$\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=$MlfDir$\TIMIT.statelist
      ]
]
TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=$ExpDir$\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
]
TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=$ExpDir$\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=$MelDir$\add_layer.mel
]
TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
]
TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=$ExpDir$\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=$MelDir$\add_layer.mel
]
TIMIT_Train3=[
    action=train
    modelPath=$ExpDir$\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
]
currentDirectory=D:\TestPreparation\Speech\ASR
RunDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu
DataDir=D:\TestPreparation\Speech\ASR
ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
OutputDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu
DeviceId=0
timestamping=true
LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
ScpDir=D:\TestPreparation\Speech\ASR
MlfDir=D:\TestPreparation\Speech\ASR
NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
ExpDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp
DeviceNumber=0
SGD=[maxEpochs=1]
SGD=[epochSize=2048]
TIMIT_Train3=[SGD=[maxEpochs=1]]
TIMIT_Train3=[SGD=[epochSize=2048]]
reader=[verbosity=0]
numMBsToShowResult=5

04/06/2016 10:17:32: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

04/06/2016 10:17:32: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
04/06/2016 10:17:32: command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
precision=float
ndlMacros=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\default_macros.ndl
deviceId=0
traceLevel=1
SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
]
reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=D:\TestPreparation\Speech\ASR\TIMIT.statelist
      ]
]
TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]
TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]
TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]
TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]
TIMIT_Train3=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
]
currentDirectory=D:\TestPreparation\Speech\ASR
RunDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu
DataDir=D:\TestPreparation\Speech\ASR
ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
OutputDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu
DeviceId=0
timestamping=true
LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
ScpDir=D:\TestPreparation\Speech\ASR
MlfDir=D:\TestPreparation\Speech\ASR
NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
ExpDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp
DeviceNumber=0
SGD=[maxEpochs=1]
SGD=[epochSize=2048]
TIMIT_Train3=[SGD=[maxEpochs=1]]
TIMIT_Train3=[SGD=[epochSize=2048]]
reader=[verbosity=0]
numMBsToShowResult=5

04/06/2016 10:17:32: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

04/06/2016 10:17:32: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: TIMIT_TrainWithPreTrain.cntk:command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
configparameters: TIMIT_TrainWithPreTrain.cntk:ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:currentDirectory=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:DataDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:deviceId=0
configparameters: TIMIT_TrainWithPreTrain.cntk:DeviceNumber=0
configparameters: TIMIT_TrainWithPreTrain.cntk:ExpDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp
configparameters: TIMIT_TrainWithPreTrain.cntk:LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
configparameters: TIMIT_TrainWithPreTrain.cntk:MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:MlfDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:ndlMacros=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\default_macros.ndl
configparameters: TIMIT_TrainWithPreTrain.cntk:numMBsToShowResult=5
configparameters: TIMIT_TrainWithPreTrain.cntk:OutputDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu
configparameters: TIMIT_TrainWithPreTrain.cntk:precision=float
configparameters: TIMIT_TrainWithPreTrain.cntk:reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=D:\TestPreparation\Speech\ASR\TIMIT.statelist
      ]
] [verbosity=0]

configparameters: TIMIT_TrainWithPreTrain.cntk:RunDir=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu
configparameters: TIMIT_TrainWithPreTrain.cntk:ScpDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
] [maxEpochs=1] [epochSize=2048]

configparameters: TIMIT_TrainWithPreTrain.cntk:timestamping=true
configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_Train3=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
] [SGD=[maxEpochs=1]] [SGD=[epochSize=2048]]

configparameters: TIMIT_TrainWithPreTrain.cntk:traceLevel=1
04/06/2016 10:17:32: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
04/06/2016 10:17:32: Commands: TIMIT_DiscrimPreTrain1 TIMIT_AddLayer2 TIMIT_DiscrimPreTrain2 TIMIT_AddLayer3 TIMIT_Train3
04/06/2016 10:17:32: Precision = "float"
04/06/2016 10:17:32: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
04/06/2016 10:17:32: CNTKCommandTrainInfo: TIMIT_DiscrimPreTrain1 : 1
04/06/2016 10:17:32: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
04/06/2016 10:17:32: CNTKCommandTrainInfo: TIMIT_DiscrimPreTrain2 : 1
04/06/2016 10:17:32: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
04/06/2016 10:17:32: CNTKCommandTrainInfo: TIMIT_Train3 : 1
04/06/2016 10:17:32: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3

04/06/2016 10:17:32: ##############################################################################
04/06/2016 10:17:32: #                                                                            #
04/06/2016 10:17:32: # Action "train"                                                             #
04/06/2016 10:17:32: #                                                                            #
04/06/2016 10:17:32: ##############################################################################

04/06/2016 10:17:32: CNTKCommandTrainBegin: TIMIT_DiscrimPreTrain1
NDLBuilder Using GPU 0
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:17:42: Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 19 nodes to process in pass 1.


Validating network. 13 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L1.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:17:43: Created model with 19 nodes on GPU 0.

04/06/2016 10:17:43: Training criterion node(s):
04/06/2016 10:17:43: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:17:43: Evaluation criterion node(s):

04/06/2016 10:17:43: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

04/06/2016 10:17:43: Precomputing --> 3 PreCompute nodes found.

04/06/2016 10:17:43: 	featNorm.xMean = Mean()
04/06/2016 10:17:43: 	featNorm.xStdDev = InvStdDev()
04/06/2016 10:17:43: 	logPrior.Prior = Mean()
minibatchiterator: epoch 0: frames [0..1124823] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:19:25: Precomputing --> Completed.


04/06/2016 10:19:25: Starting Epoch 1: learning rate per sample = 0.000391  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses

04/06/2016 10:19:25: Starting minibatch loop.
04/06/2016 10:19:25:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  5.26064072; EvalErr[0]PerSample = 0.99687500; TotalTime = 0.1376s; SamplesPerSecond = 9304.2
04/06/2016 10:19:25: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 5.2157345; TotalSamplesSeen = 2048; EvalErrPerSample = 0.98974609; AvgLearningRatePerSample = 0.00039062501; EpochTime=0.227603
04/06/2016 10:19:25: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn'
04/06/2016 10:19:25: CNTKCommandTrainEnd: TIMIT_DiscrimPreTrain1

04/06/2016 10:19:25: Action "train" complete.


04/06/2016 10:19:25: ##############################################################################
04/06/2016 10:19:25: #                                                                            #
04/06/2016 10:19:25: # Action "edit"                                                              #
04/06/2016 10:19:25: #                                                                            #
04/06/2016 10:19:25: ##############################################################################


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 19 nodes to process in pass 1.


Validating network. 13 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L1.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 12 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


04/06/2016 10:19:26: Action "edit" complete.


04/06/2016 10:19:26: ##############################################################################
04/06/2016 10:19:26: #                                                                            #
04/06/2016 10:19:26: # Action "train"                                                             #
04/06/2016 10:19:26: #                                                                            #
04/06/2016 10:19:26: ##############################################################################

04/06/2016 10:19:26: CNTKCommandTrainBegin: TIMIT_DiscrimPreTrain2
NDLBuilder Using GPU 0
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:19:35: Starting from checkpoint. Loading network from 'F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0'.

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 16 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:19:35: Loaded model with 24 nodes on GPU 0.

04/06/2016 10:19:35: Training criterion node(s):
04/06/2016 10:19:35: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:19:35: Evaluation criterion node(s):

04/06/2016 10:19:35: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
04/06/2016 10:19:35: No PreCompute nodes found, skipping PreCompute step.

04/06/2016 10:19:35: Starting Epoch 1: learning rate per sample = 0.000391  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:20:02: Starting minibatch loop.
04/06/2016 10:20:02:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  5.04676819; EvalErr[0]PerSample = 0.95781250; TotalTime = 0.1455s; SamplesPerSecond = 8794.9
04/06/2016 10:20:02: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 5.0271077; TotalSamplesSeen = 2048; EvalErrPerSample = 0.95263672; AvgLearningRatePerSample = 0.00039062501; EpochTime=27.394
04/06/2016 10:20:02: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn'
04/06/2016 10:20:03: CNTKCommandTrainEnd: TIMIT_DiscrimPreTrain2

04/06/2016 10:20:03: Action "train" complete.


04/06/2016 10:20:03: ##############################################################################
04/06/2016 10:20:03: #                                                                            #
04/06/2016 10:20:03: # Action "edit"                                                              #
04/06/2016 10:20:03: #                                                                            #
04/06/2016 10:20:03: ##############################################################################


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 16 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 29 nodes to process in pass 1.


Validating network. 15 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L3.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L3.BFF.FF.T = Times (L3.BFF.W, L2.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L3.BFF.B = LearnableParameter() :  -> [512]
Validating --> L3.BFF.FF.P = Plus (L3.BFF.FF.T, L3.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L3.S = Sigmoid (L3.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L3.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


04/06/2016 10:20:03: Action "edit" complete.


04/06/2016 10:20:03: ##############################################################################
04/06/2016 10:20:03: #                                                                            #
04/06/2016 10:20:03: # Action "train"                                                             #
04/06/2016 10:20:03: #                                                                            #
04/06/2016 10:20:03: ##############################################################################

04/06/2016 10:20:03: CNTKCommandTrainBegin: TIMIT_Train3
NDLBuilder Using GPU 0
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:20:11: Starting from checkpoint. Loading network from 'F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0'.

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 29 nodes to process in pass 1.


Validating network. 19 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L3.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L3.BFF.FF.T = Times (L3.BFF.W, L2.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L3.BFF.B = LearnableParameter() :  -> [512]
Validating --> L3.BFF.FF.P = Plus (L3.BFF.FF.T, L3.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L3.S = Sigmoid (L3.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L3.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:20:12: Loaded model with 29 nodes on GPU 0.

04/06/2016 10:20:12: Training criterion node(s):
04/06/2016 10:20:12: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:20:12: Evaluation criterion node(s):

04/06/2016 10:20:12: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
04/06/2016 10:20:12: No PreCompute nodes found, skipping PreCompute step.

04/06/2016 10:20:12: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:20:39: Starting minibatch loop.
04/06/2016 10:20:39:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  4.96178207; EvalErr[0]PerSample = 0.96250000; TotalTime = 0.1611s; SamplesPerSecond = 7944.8
04/06/2016 10:20:39: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 4.9537888; TotalSamplesSeen = 2048; EvalErrPerSample = 0.95556641; AvgLearningRatePerSample = 0.003125; EpochTime=27.3611
04/06/2016 10:20:39: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406111729.148509\Examples\Speech\TIMIT_TrainWithPreTrain@debug_gpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn'
04/06/2016 10:20:39: CNTKCommandTrainEnd: TIMIT_Train3

04/06/2016 10:20:39: Action "train" complete.

04/06/2016 10:20:39: __COMPLETED__