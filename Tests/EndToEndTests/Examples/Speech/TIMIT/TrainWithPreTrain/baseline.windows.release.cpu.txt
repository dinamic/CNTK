=== Running /cygdrive/c/repo/cntk_github6/CNTK/x64/release/cntk.exe configFile=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/TIMIT_TrainWithPreTrain.cntk currentDirectory=D:\TestPreparation\Speech\ASR RunDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu DataDir=D:\TestPreparation\Speech\ASR ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config OutputDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu DeviceId=-1 timestamping=true LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib ScpDir=D:\TestPreparation\Speech\ASR MlfDir=D:\TestPreparation\Speech\ASR NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config ExpDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp DeviceNumber=-1 SGD=[maxEpochs=1] SGD=[epochSize=2048] TIMIT_Train3=[SGD=[maxEpochs=1]] TIMIT_Train3=[SGD=[epochSize=2048]] reader=[verbosity=0] numMBsToShowResult=5
-------------------------------------------------------------------
Build info: 

		Built time: Apr  6 2016 09:34:06
		Last modified date: Tue Apr  5 15:50:31 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: yes
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
		CUB_PATH: c:\Tools\cub-1.4.1\
		CUDNN_PATH: c:\Tools\cudnn-4.0\cuda
		Build Branch: eldak/addingTimitExamplesToTests
		Build SHA1: 40db0b8fcc1e6ffd89b9357a636439c63130b589 (modified)
		Built by eldak on ELDAK-0
		Build Path: c:\repo\cntk_github6\CNTK\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to D:\TestPreparation\Speech\ASR
04/06/2016 10:15:48: -------------------------------------------------------------------
04/06/2016 10:15:48: Build info: 

04/06/2016 10:15:48: 		Built time: Apr  6 2016 09:34:06
04/06/2016 10:15:48: 		Last modified date: Tue Apr  5 15:50:31 2016
04/06/2016 10:15:48: 		Build type: Release
04/06/2016 10:15:48: 		Build target: GPU
04/06/2016 10:15:48: 		With 1bit-SGD: yes
04/06/2016 10:15:48: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
04/06/2016 10:15:48: 		CUB_PATH: c:\Tools\cub-1.4.1\
04/06/2016 10:15:48: 		CUDNN_PATH: c:\Tools\cudnn-4.0\cuda
04/06/2016 10:15:48: 		Build Branch: eldak/addingTimitExamplesToTests
04/06/2016 10:15:48: 		Build SHA1: 40db0b8fcc1e6ffd89b9357a636439c63130b589 (modified)
04/06/2016 10:15:48: 		Built by eldak on ELDAK-0
04/06/2016 10:15:48: 		Build Path: c:\repo\cntk_github6\CNTK\Source\CNTK\
04/06/2016 10:15:48: -------------------------------------------------------------------

04/06/2016 10:15:48: Running on ELDAK-0 at 2016/04/06 10:15:48
04/06/2016 10:15:48: Command line: 
C:\repo\cntk_github6\CNTK\x64\release\cntk.exe  configFile=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/TIMIT_TrainWithPreTrain.cntk  currentDirectory=D:\TestPreparation\Speech\ASR  RunDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu  DataDir=D:\TestPreparation\Speech\ASR  ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  OutputDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu  DeviceId=-1  timestamping=true  LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib  ScpDir=D:\TestPreparation\Speech\ASR  MlfDir=D:\TestPreparation\Speech\ASR  NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config  ExpDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp  DeviceNumber=-1  SGD=[maxEpochs=1]  SGD=[epochSize=2048]  TIMIT_Train3=[SGD=[maxEpochs=1]]  TIMIT_Train3=[SGD=[epochSize=2048]]  reader=[verbosity=0]  numMBsToShowResult=5



04/06/2016 10:15:48: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
04/06/2016 10:15:48: command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
precision=float
ndlMacros=$NdlDir$\default_macros.ndl
deviceId=$DeviceNumber$
traceLevel=1
SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
]
reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=$ScpDir$\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=$MlfDir$\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=$MlfDir$\TIMIT.statelist
      ]
]
TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=$ExpDir$\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
]
TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=$ExpDir$\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=$MelDir$\add_layer.mel
]
TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
]
TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=$ExpDir$\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=$ExpDir$\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=$MelDir$\add_layer.mel
]
TIMIT_Train3=[
    action=train
    modelPath=$ExpDir$\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=$NdlDir$\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
]
currentDirectory=D:\TestPreparation\Speech\ASR
RunDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu
DataDir=D:\TestPreparation\Speech\ASR
ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
OutputDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu
DeviceId=-1
timestamping=true
LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
ScpDir=D:\TestPreparation\Speech\ASR
MlfDir=D:\TestPreparation\Speech\ASR
NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
ExpDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp
DeviceNumber=-1
SGD=[maxEpochs=1]
SGD=[epochSize=2048]
TIMIT_Train3=[SGD=[maxEpochs=1]]
TIMIT_Train3=[SGD=[epochSize=2048]]
reader=[verbosity=0]
numMBsToShowResult=5

04/06/2016 10:15:48: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

04/06/2016 10:15:48: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
04/06/2016 10:15:48: command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
precision=float
ndlMacros=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\default_macros.ndl
deviceId=-1
traceLevel=1
SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
]
reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=D:\TestPreparation\Speech\ASR\TIMIT.statelist
      ]
]
TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]
TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]
TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]
TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]
TIMIT_Train3=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
]
currentDirectory=D:\TestPreparation\Speech\ASR
RunDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu
DataDir=D:\TestPreparation\Speech\ASR
ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
OutputDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu
DeviceId=-1
timestamping=true
LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
ScpDir=D:\TestPreparation\Speech\ASR
MlfDir=D:\TestPreparation\Speech\ASR
NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
ExpDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp
DeviceNumber=-1
SGD=[maxEpochs=1]
SGD=[epochSize=2048]
TIMIT_Train3=[SGD=[maxEpochs=1]]
TIMIT_Train3=[SGD=[epochSize=2048]]
reader=[verbosity=0]
numMBsToShowResult=5

04/06/2016 10:15:48: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

04/06/2016 10:15:48: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: TIMIT_TrainWithPreTrain.cntk:command=TIMIT_DiscrimPreTrain1:TIMIT_AddLayer2:TIMIT_DiscrimPreTrain2:TIMIT_AddLayer3:TIMIT_Train3
configparameters: TIMIT_TrainWithPreTrain.cntk:ConfigDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:currentDirectory=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:DataDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:deviceId=-1
configparameters: TIMIT_TrainWithPreTrain.cntk:DeviceNumber=-1
configparameters: TIMIT_TrainWithPreTrain.cntk:ExpDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp
configparameters: TIMIT_TrainWithPreTrain.cntk:LibDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config/../lib
configparameters: TIMIT_TrainWithPreTrain.cntk:MelDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:MlfDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:NdlDir=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config
configparameters: TIMIT_TrainWithPreTrain.cntk:ndlMacros=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\default_macros.ndl
configparameters: TIMIT_TrainWithPreTrain.cntk:numMBsToShowResult=5
configparameters: TIMIT_TrainWithPreTrain.cntk:OutputDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu
configparameters: TIMIT_TrainWithPreTrain.cntk:precision=float
configparameters: TIMIT_TrainWithPreTrain.cntk:reader=[
      readerType=HTKMLFReader
      readMethod=blockRandomize
      miniBatchMode=Partial
      randomize=Auto
      verbosity=1   
      features=[
	  dim=792
	  scpFile=D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn
      ]
      labels=[
	mlfFile=D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk
        labelDim=183
	labelMappingFile=D:\TestPreparation\Speech\ASR\TIMIT.statelist
      ]
] [verbosity=0]

configparameters: TIMIT_TrainWithPreTrain.cntk:RunDir=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu
configparameters: TIMIT_TrainWithPreTrain.cntk:ScpDir=D:\TestPreparation\Speech\ASR
configparameters: TIMIT_TrainWithPreTrain.cntk:SGD=[
        epochSize=0 
        minibatchSize=256
        learningRatesPerMB=0.1
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=2
] [maxEpochs=1] [epochSize=2048]

configparameters: TIMIT_TrainWithPreTrain.cntk:timestamping=true
configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_AddLayer3=[
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NewModel=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0
    editPath=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\add_layer.mel
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_DiscrimPreTrain1=[
    action=train    
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_DiscrimPreTrain2=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
]

configparameters: TIMIT_TrainWithPreTrain.cntk:TIMIT_Train3=[
    action=train
    modelPath=F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
    NDLNetworkBuilder=[
	NetworkDescription=C:\repo\cntk_github6\CNTK\Examples\Speech\Miscellaneous\TIMIT\config\create_1layer.ndl
    ]
    SGD=[
        epochSize=0 
        minibatchSize=256:1024
        learningRatesPerMB=0.8:3.2*14:0.08
        momentumPerMB=0.9
        dropoutRate=0.0
        maxEpochs=25
    ]  
] [SGD=[maxEpochs=1]] [SGD=[epochSize=2048]]

configparameters: TIMIT_TrainWithPreTrain.cntk:traceLevel=1
04/06/2016 10:15:48: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
04/06/2016 10:15:48: Commands: TIMIT_DiscrimPreTrain1 TIMIT_AddLayer2 TIMIT_DiscrimPreTrain2 TIMIT_AddLayer3 TIMIT_Train3
04/06/2016 10:15:48: Precision = "float"
04/06/2016 10:15:48: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn
04/06/2016 10:15:48: CNTKCommandTrainInfo: TIMIT_DiscrimPreTrain1 : 1
04/06/2016 10:15:48: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn
04/06/2016 10:15:48: CNTKCommandTrainInfo: TIMIT_DiscrimPreTrain2 : 1
04/06/2016 10:15:48: CNTKModelPath: F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn
04/06/2016 10:15:48: CNTKCommandTrainInfo: TIMIT_Train3 : 1
04/06/2016 10:15:48: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3

04/06/2016 10:15:48: ##############################################################################
04/06/2016 10:15:48: #                                                                            #
04/06/2016 10:15:48: # Action "train"                                                             #
04/06/2016 10:15:48: #                                                                            #
04/06/2016 10:15:48: ##############################################################################

04/06/2016 10:15:48: CNTKCommandTrainBegin: TIMIT_DiscrimPreTrain1
NDLBuilder Using CPU
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:15:49: Creating virgin network.

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 19 nodes to process in pass 1.


Validating network. 13 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L1.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:15:49: Created model with 19 nodes on CPU.

04/06/2016 10:15:49: Training criterion node(s):
04/06/2016 10:15:49: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:15:49: Evaluation criterion node(s):

04/06/2016 10:15:49: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

04/06/2016 10:15:49: Precomputing --> 3 PreCompute nodes found.

04/06/2016 10:15:49: 	featNorm.xMean = Mean()
04/06/2016 10:15:49: 	featNorm.xStdDev = InvStdDev()
04/06/2016 10:15:49: 	logPrior.Prior = Mean()
minibatchiterator: epoch 0: frames [0..1124823] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:16:00: Precomputing --> Completed.


04/06/2016 10:16:00: Starting Epoch 1: learning rate per sample = 0.000391  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses

04/06/2016 10:16:00: Starting minibatch loop.
04/06/2016 10:16:00:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  5.28307190; EvalErr[0]PerSample = 0.99609375; TotalTime = 0.1600s; SamplesPerSecond = 7999.3
04/06/2016 10:16:00: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 5.2337179; TotalSamplesSeen = 2048; EvalErrPerSample = 0.98779297; AvgLearningRatePerSample = 0.00039062501; EpochTime=0.213869
04/06/2016 10:16:00: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel1\cntkSpeech.dnn'
04/06/2016 10:16:00: CNTKCommandTrainEnd: TIMIT_DiscrimPreTrain1

04/06/2016 10:16:00: Action "train" complete.


04/06/2016 10:16:00: ##############################################################################
04/06/2016 10:16:00: #                                                                            #
04/06/2016 10:16:00: # Action "edit"                                                              #
04/06/2016 10:16:00: #                                                                            #
04/06/2016 10:16:00: ##############################################################################


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 19 nodes to process in pass 1.


Validating network. 13 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L1.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 12 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


04/06/2016 10:16:00: Action "edit" complete.


04/06/2016 10:16:00: ##############################################################################
04/06/2016 10:16:00: #                                                                            #
04/06/2016 10:16:00: # Action "train"                                                             #
04/06/2016 10:16:00: #                                                                            #
04/06/2016 10:16:00: ##############################################################################

04/06/2016 10:16:00: CNTKCommandTrainBegin: TIMIT_DiscrimPreTrain2
NDLBuilder Using CPU
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:16:01: Starting from checkpoint. Loading network from 'F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn.0'.

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 16 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:16:01: Loaded model with 24 nodes on CPU.

04/06/2016 10:16:01: Training criterion node(s):
04/06/2016 10:16:01: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:16:01: Evaluation criterion node(s):

04/06/2016 10:16:01: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
04/06/2016 10:16:01: No PreCompute nodes found, skipping PreCompute step.

04/06/2016 10:16:01: Starting Epoch 1: learning rate per sample = 0.000391  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:16:03: Starting minibatch loop.
04/06/2016 10:16:03:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  5.07689781; EvalErr[0]PerSample = 0.96015625; TotalTime = 0.1401s; SamplesPerSecond = 9133.2
04/06/2016 10:16:03: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 5.053731; TotalSamplesSeen = 2048; EvalErrPerSample = 0.95507813; AvgLearningRatePerSample = 0.00039062501; EpochTime=1.51288
04/06/2016 10:16:03: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\dptmodel2\cntkSpeech.dnn'
04/06/2016 10:16:03: CNTKCommandTrainEnd: TIMIT_DiscrimPreTrain2

04/06/2016 10:16:03: Action "train" complete.


04/06/2016 10:16:03: ##############################################################################
04/06/2016 10:16:03: #                                                                            #
04/06/2016 10:16:03: # Action "edit"                                                              #
04/06/2016 10:16:03: #                                                                            #
04/06/2016 10:16:03: ##############################################################################


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 24 nodes to process in pass 1.


Validating network. 16 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L2.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 29 nodes to process in pass 1.


Validating network. 15 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L3.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L3.BFF.FF.T = Times (L3.BFF.W, L2.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L3.BFF.B = LearnableParameter() :  -> [512]
Validating --> L3.BFF.FF.P = Plus (L3.BFF.FF.T, L3.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L3.S = Sigmoid (L3.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L3.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


04/06/2016 10:16:03: Action "edit" complete.


04/06/2016 10:16:03: ##############################################################################
04/06/2016 10:16:03: #                                                                            #
04/06/2016 10:16:03: # Action "train"                                                             #
04/06/2016 10:16:03: #                                                                            #
04/06/2016 10:16:03: ##############################################################################

04/06/2016 10:16:03: CNTKCommandTrainBegin: TIMIT_Train3
NDLBuilder Using CPU
reading script file D:\TestPreparation\Speech\ASR\TIMIT.train.scp.fbank.fullpath.rnn ... 3696 entries
total 183 state names in state list D:\TestPreparation\Speech\ASR\TIMIT.statelist
htkmlfreader: reading MLF file D:\TestPreparation\Speech\ASR\TIMIT.train.align_cistate.mlf.cntk ... total 3696 entries
....................................................................................................feature set 0: 1124823 frames in 3696 out of 3696 utterances
label set 0: 183 classes
minibatchutterancesource: 3696 utterances grouped into 13 chunks, av. chunk size: 284.3 utterances, 86524.8 frames

04/06/2016 10:16:04: Starting from checkpoint. Loading network from 'F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn.0'.

Post-processing network...

6 roots:
	CE.SM = CrossEntropyWithSoftmax()
	Err = ErrorPrediction()
	ScaledLogLikelihood = Minus()
	featNorm.xMean = Mean()
	featNorm.xStdDev = InvStdDev()
	logPrior.Prior = Mean()

Validating network. 29 nodes to process in pass 1.


Validating network. 19 nodes to process in pass 2.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [183 x *]
Validating --> CE.BFF.W = LearnableParameter() :  -> [183 x 512]
Validating --> L3.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L2.BFF.W = LearnableParameter() :  -> [512 x 512]
Validating --> L1.BFF.W = LearnableParameter() :  -> [512 x 792]
Validating --> features = InputValue() :  -> [792 x *]
Validating --> featNorm.xMean = Mean (features) : [792 x *] -> [792]
Validating --> featNorm.xStdDev = InvStdDev (features) : [792 x *] -> [792]
Validating --> featNorm.xNorm = PerDimMeanVarNormalization (features, featNorm.xMean, featNorm.xStdDev) : [792 x *], [792], [792] -> [792 x *]
Validating --> L1.BFF.FF.T = Times (L1.BFF.W, featNorm.xNorm) : [512 x 792], [792 x *] -> [512 x *]
Validating --> L1.BFF.B = LearnableParameter() :  -> [512]
Validating --> L1.BFF.FF.P = Plus (L1.BFF.FF.T, L1.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L1.S = Sigmoid (L1.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L2.BFF.FF.T = Times (L2.BFF.W, L1.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L2.BFF.B = LearnableParameter() :  -> [512]
Validating --> L2.BFF.FF.P = Plus (L2.BFF.FF.T, L2.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L2.S = Sigmoid (L2.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> L3.BFF.FF.T = Times (L3.BFF.W, L2.S) : [512 x 512], [512 x *] -> [512 x *]
Validating --> L3.BFF.B = LearnableParameter() :  -> [512]
Validating --> L3.BFF.FF.P = Plus (L3.BFF.FF.T, L3.BFF.B) : [512 x *], [512] -> [512 x *]
Validating --> L3.S = Sigmoid (L3.BFF.FF.P) : [512 x *] -> [512 x *]
Validating --> CE.BFF.FF.T = Times (CE.BFF.W, L3.S) : [183 x 512], [512 x *] -> [183 x *]
Validating --> CE.BFF.B = LearnableParameter() :  -> [183]
Validating --> CE.BFF.FF.P = Plus (CE.BFF.FF.T, CE.BFF.B) : [183 x *], [183] -> [183 x *]
Validating --> CE.SM = CrossEntropyWithSoftmax (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> Err = ErrorPrediction (labels, CE.BFF.FF.P) : [183 x *], [183 x *] -> [1]
Validating --> logPrior.Prior = Mean (labels) : [183 x *] -> [183]
Validating --> logPrior.LogPrior = Log (logPrior.Prior) : [183] -> [183]
Validating --> ScaledLogLikelihood = Minus (CE.BFF.FF.P, logPrior.LogPrior) : [183 x *], [183] -> [183 x *]


14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/06/2016 10:16:04: Loaded model with 29 nodes on CPU.

04/06/2016 10:16:04: Training criterion node(s):
04/06/2016 10:16:04: 	CE.SM = CrossEntropyWithSoftmax

04/06/2016 10:16:04: Evaluation criterion node(s):

04/06/2016 10:16:04: 	Err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
04/06/2016 10:16:04: No PreCompute nodes found, skipping PreCompute step.

04/06/2016 10:16:04: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..2048] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 72-dimensional 'FBANK_D_A_Z' with frame shift 10.0 ms

04/06/2016 10:16:06: Starting minibatch loop.
04/06/2016 10:16:06:  Epoch[ 1 of 1]-Minibatch[   1-   5, 62.50%]: SamplesSeen = 1280; TrainLossPerSample =  4.96834183; EvalErr[0]PerSample = 0.96171875; TotalTime = 0.2220s; SamplesPerSecond = 5766.2
04/06/2016 10:16:06: Finished Epoch[ 1 of 1]: [Training Set] TrainLossPerSample = 4.9595852; TotalSamplesSeen = 2048; EvalErrPerSample = 0.95507813; AvgLearningRatePerSample = 0.003125; EpochTime=1.55051
04/06/2016 10:16:06: SGD: Saving checkpoint model 'F:\cygwin64\tmp\cntk-test-20160406111547.165259\Examples\Speech\TIMIT_TrainWithPreTrain@release_cpu/exp\TrainWithPreTrain\model\cntkSpeech.dnn'
04/06/2016 10:16:06: CNTKCommandTrainEnd: TIMIT_Train3

04/06/2016 10:16:06: Action "train" complete.

04/06/2016 10:16:06: __COMPLETED__